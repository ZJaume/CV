{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Práctica 2 CIFAR",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IR1epPYX4WR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "wnUmRVKI3p_-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importando las capas necesarias para este problema"
      ]
    },
    {
      "metadata": {
        "id": "T2tMYtcu4Yho",
        "colab_type": "code",
        "outputId": "eb9f6fcb-a836-427c-8a41-7eda45b6063f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, PReLU\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qGwx9oTj4aNc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "id": "e7spwv5k4CQA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Descargar el dataset de amazon y preprocesarlo para darle la forma que requiere la red. Cambiar el tipo de dato a float de 32 bits y normalizar a valores entre 0 y 1 para las imágenes de entrada. Para las salidas transoformarlas a \"one-hot\" vector."
      ]
    },
    {
      "metadata": {
        "id": "fZMY8tN_4crQ",
        "colab_type": "code",
        "outputId": "7f48a88e-ad0d-4f30-cd9c-823b69dad679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "num_classes=10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 25s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M-DICVhe_VZg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation\n"
      ]
    },
    {
      "metadata": {
        "id": "cesEaqjw_YAs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "  featurewise_center=True,\n",
        "  featurewise_std_normalization=True,\n",
        "  width_shift_range=0.2,\n",
        "  height_shift_range=0.2,\n",
        "  rotation_range=20,\n",
        "  zoom_range=[1.0,1.2],\n",
        "  horizontal_flip=True)\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "testdatagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        ")\n",
        "\n",
        "testdatagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1gKee5mtD9IY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Callbacks"
      ]
    },
    {
      "metadata": {
        "id": "Cz_C-tUkCgvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Enfriamiento LR"
      ]
    },
    {
      "metadata": {
        "id": "Kcr6wAo85vFq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se prueban distintas técicas de enfriamiento del factor de aprendizaje.\n",
        "\n",
        "\n",
        "1.   Decaimiento por pasos, reducir a la mitad cada 20 epocas.\n",
        "2.   Enfriamiento programado donde especificamos en que época concreta queremos bajar el factor de aprendizaje y a que valor concreto.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pJ9mNDAFCkNc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "import math\n",
        "\n",
        "# learning rate schedule\n",
        "def step_decay(epoch,lr):\n",
        "\tinitial_lrate = 0.1\n",
        "\tdrop = 0.5\n",
        "\tepochs_drop = 20.0\n",
        "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "\treturn lrate\n",
        "\n",
        "def scheduler(epoch):\n",
        "  if epoch < 25:\n",
        "    return .1\n",
        "  elif epoch < 50:\n",
        "    return 0.01\n",
        "  else:\n",
        "    return 0.001\n",
        "  \n",
        "def nin_scheduler(epoch):\n",
        "  if epoch <= 60:\n",
        "      return 0.05\n",
        "  if epoch <= 120:\n",
        "      return 0.01\n",
        "  if epoch <= 160:\n",
        "      return 0.002\n",
        "  return 0.0004\n",
        "\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "lrate_nin = LearningRateScheduler(nin_scheduler)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              min_lr=0.0001,\n",
        "                              patience=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Uq5L4PYEEBA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Guardar modelos"
      ]
    },
    {
      "metadata": {
        "id": "aPGlKBkPA0ZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Callback para que guarde el modelo seiempre que mejore la precisión en el conjunto de validación."
      ]
    },
    {
      "metadata": {
        "id": "ljcf6BiSEHp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "chkpoint = ModelCheckpoint('nin-{epoch:03d}-{val_acc:.2f}.hdf5', monitor='val_acc', save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HI8TlWIH4y8Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Topologia"
      ]
    },
    {
      "metadata": {
        "id": "bMPEZFFr4h-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bloque convolucional"
      ]
    },
    {
      "metadata": {
        "id": "R_GmPeGxA9P2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Función que construye un bloque convolucional. Primero una convolución de 3x3 y posteriormente aplicar la normalización del batch, el ruido gausiano, la activación y finalmente el resumen."
      ]
    },
    {
      "metadata": {
        "id": "AUfuXmE64nr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## DEF A BLOCK CONV + BN + GN + MAXPOOL\n",
        "def CBGN(model,filters,ishape=0):\n",
        "  if (ishape!=0):\n",
        "    model.add(Conv2D(filters, (3, 3), padding='same',\n",
        "                 input_shape=ishape))\n",
        "  else:\n",
        "    model.add(Conv2D(filters, (3, 3), padding='same'))\n",
        "  \n",
        "  model.add(BN())\n",
        "  model.add(GN(0.3))\n",
        "  model.add(PReLU())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u-fJlTnpw5Xw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ConvNN"
      ]
    },
    {
      "metadata": {
        "id": "ucRkFPi5BWBg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Definición de la arquitectura. 5 bloques convolucionales con un número de filtros que aumenta con la profundidad de la red. Posteriormente aplanar el vector de salida de la parte convolucional y pasarlo  a las capas densas. Estas a su vez se les aplica el ruido gaussiano y la normalización.\n",
        "Para las activaciones se usa la ReLU parametrizada, para aprender el "
      ]
    },
    {
      "metadata": {
        "id": "tFaLmsPc41ew",
        "colab_type": "code",
        "outputId": "aeed3f0a-6fe4-4a69-cd2a-3184fe15905c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model=CBGN(model,32,x_train.shape[1:])\n",
        "model=CBGN(model,64)\n",
        "model=CBGN(model,128)\n",
        "model=CBGN(model,256)\n",
        "model=CBGN(model,512)\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(PReLU())\n",
        "\n",
        "model.add(BN())\n",
        "model.add(GN(0.3))\n",
        "model.add(Dense(1024))\n",
        "model.add(PReLU())\n",
        "\n",
        "model.add(BN())\n",
        "model.add(GN(0.3))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_1 (GaussianNo (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "p_re_lu_1 (PReLU)            (None, 32, 32, 32)        32768     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_2 (GaussianNo (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "p_re_lu_2 (PReLU)            (None, 16, 16, 64)        16384     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_3 (GaussianNo (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "p_re_lu_3 (PReLU)            (None, 8, 8, 128)         8192      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_4 (GaussianNo (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "p_re_lu_4 (PReLU)            (None, 4, 4, 256)         4096      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 2, 2, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_5 (GaussianNo (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "p_re_lu_5 (PReLU)            (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "p_re_lu_6 (PReLU)            (None, 1024)              1024      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_6 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "p_re_lu_7 (PReLU)            (None, 1024)              1024      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_7 (GaussianNo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10250     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 3,231,434\n",
            "Trainable params: 3,225,354\n",
            "Non-trainable params: 6,080\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9LfAcgbuxBmU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## NiN"
      ]
    },
    {
      "metadata": {
        "id": "iQcFD2dFbmW8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implementación del modelo Network In Network con convoluciones de 1x1.\n",
        "Por un lado la funcion que crea un bloque con convolucion normal, y despues variso bloques de convolucion de 1x1.\n",
        "Postetiormente la funcion que agrupa los bloques."
      ]
    },
    {
      "metadata": {
        "id": "c9ra1p9dxFs0",
        "colab_type": "code",
        "outputId": "c67211dd-235e-4fea-a239-135e21086565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Reshape, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "weight_decay = 0.0001\n",
        "\n",
        "def nin_block(kernel, mlps, strides=[1,1]):\n",
        "    def inner(x):\n",
        "      l = Conv2D(mlps[0], kernel, strides=strides, padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer='he_normal')(x)\n",
        "      l = BN()(l)\n",
        "      l = Activation('relu')(l)\n",
        "      for size in mlps[1:]:\n",
        "        l = Conv2D(size, 1, strides=[1,1], padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer='he_normal')(l)\n",
        "        l = BN()(l)\n",
        "        l = Activation('relu')(l)\n",
        "      return l\n",
        "    return inner\n",
        "\n",
        "def NetworkInNetwork(inshape):\n",
        "  inpt = Input(shape = inshape)\n",
        "  x = nin_block(5,[192,160,96])(inpt)\n",
        "  x = MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  \n",
        "  x = nin_block(5,[192,192,192])(x)\n",
        "  x = MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  \n",
        "  x = nin_block(3,[192,192,num_classes])(x)\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  #x = Dense(num_classes)(x)\n",
        "  output = Activation('softmax')(x)\n",
        "  return Model(inpt,output)\n",
        "NiN = NetworkInNetwork(x_train.shape[1:])\n",
        "NiN.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 192)       14592     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32, 32, 192)       768       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 160)       30880     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 32, 32, 160)       640       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32, 32, 160)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 96)        15456     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 32, 32, 96)        384       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 32, 32, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 192)       460992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 16, 16, 192)       37056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 192)       37056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 16, 16, 192)       768       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16, 16, 192)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 192)         331968    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 8, 8, 192)         37056     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 8, 8, 192)         768       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8, 8, 192)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 8, 8, 10)          1930      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 8, 8, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 8, 8, 10)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 972,658\n",
            "Trainable params: 969,822\n",
            "Non-trainable params: 2,836\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cl0XzmnGQVgo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ]
    },
    {
      "metadata": {
        "id": "D8kMoCfpQhtg",
        "colab_type": "code",
        "outputId": "fe7e2f4f-a351-4f31-9300-730a5e567d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5505
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Add, ZeroPadding2D\n",
        "\n",
        "def main_path(convs, identity=True):\n",
        "    def inner(x):\n",
        "        if not identity:\n",
        "            l = Conv2D(convs, 3, strides=(2,2), padding='same', kernel_initializer='he_normal')(x)\n",
        "        else:\n",
        "            l = Conv2D(convs, 3, strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
        "        l = BN()(l) \n",
        "        l = Activation('relu')(l)\n",
        "\n",
        "        l = Conv2D(convs, 3, strides=(1,1), padding='same', kernel_initializer='he_normal')(l)\n",
        "        l = BN()(l) \n",
        "        l = Activation('relu')(l)\n",
        "        \n",
        "        return l\n",
        "    return inner\n",
        "        \n",
        "def res_block(convs, identity=True):\n",
        "    def inner(x):\n",
        "        if not identity:\n",
        "            shortcut = Conv2D(convs, 1, strides=(2,2), padding='same', kernel_initializer='he_normal')(x)\n",
        "            shortcut = BN()(shortcut)\n",
        "            print(shortcut)\n",
        "            l = Add()([main_path(convs,False)(x),shortcut])\n",
        "        else:\n",
        "            l = Add()([main_path(convs)(x),x])\n",
        "        l = Activation('relu')(l)\n",
        "        return l\n",
        "    return inner\n",
        "\n",
        "\n",
        "def resnet(input_shape):\n",
        "    inpt = Input(shape = input_shape)\n",
        "    print(inpt)\n",
        "\n",
        "    # stage 1\n",
        "    x = Conv2D(16, 3, strides=(1,1), padding='same', kernel_initializer='he_normal')(inpt)\n",
        "    x = BN()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    #x = MaxPooling2D(pool_size=(3,3),strides=2)(x)\n",
        "    print(x)\n",
        "    \n",
        "    # stage 2\n",
        "    x = res_block(16)(x)\n",
        "    x = res_block(16)(x)\n",
        "    x = res_block(16)(x)\n",
        "    print(x)\n",
        "\n",
        "    # stage 3\n",
        "    x = res_block(32,False)(x)\n",
        "    x = res_block(32)(x)\n",
        "    x = res_block(32)(x)\n",
        "    x = res_block(32)(x)\n",
        "    print(x)\n",
        "\n",
        "    # stage 4\n",
        "    x = res_block(64,False)(x)\n",
        "    x = res_block(64)(x)\n",
        "    x = res_block(64)(x)\n",
        "    x = res_block(64)(x)\n",
        "    x = res_block(64)(x)\n",
        "    x = res_block(64)(x)\n",
        "    print(x)\n",
        "\n",
        "    # stage \n",
        "    x = res_block(128,False)(x)\n",
        "    x = res_block(128)(x)\n",
        "    x = res_block(128)(x)\n",
        "    print(x)\n",
        "\n",
        "    x = BN()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    #x = Flatten()(x)\n",
        "    outpt = Dense(num_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inpt,outpt)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def res_scheduler(epoch):\n",
        "    if epoch < 81:\n",
        "        return 0.1\n",
        "    if epoch < 122:\n",
        "        return 0.01\n",
        "    return 0.001\n",
        "\n",
        "lrate_res = LearningRateScheduler(res_scheduler)\n",
        "\n",
        "resnet = resnet(x_train.shape[1:])\n",
        "resnet.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_4:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
            "Tensor(\"activation_111/Relu:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
            "Tensor(\"activation_120/Relu:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
            "Tensor(\"batch_normalization_97/cond/Merge:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
            "Tensor(\"activation_132/Relu:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
            "Tensor(\"batch_normalization_106/cond/Merge:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"activation_150/Relu:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
            "Tensor(\"batch_normalization_119/cond/Merge:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
            "Tensor(\"activation_159/Relu:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 32, 32, 16)   448         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 32, 32, 16)   64          conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 32, 32, 16)   0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 32, 32, 16)   2320        activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 32, 32, 16)   64          conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 32, 32, 16)   0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 32, 32, 16)   2320        activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 32, 32, 16)   64          conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 32, 32, 16)   0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 32, 32, 16)   0           activation_113[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 32, 32, 16)   0           add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 32, 32, 16)   2320        activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 32, 32, 16)   64          conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 32, 32, 16)   0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 32, 32, 16)   2320        activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 32, 32, 16)   64          conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 32, 32, 16)   0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 32, 32, 16)   0           activation_116[0][0]             \n",
            "                                                                 activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 32, 32, 16)   0           add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 32, 32, 16)   2320        activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 32, 32, 16)   64          conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 32, 32, 16)   0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 32, 32, 16)   2320        activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 32, 32, 16)   64          conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 32, 32, 16)   0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 32, 32, 16)   0           activation_119[0][0]             \n",
            "                                                                 activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 32, 32, 16)   0           add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 16, 16, 32)   4640        activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 16, 16, 32)   128         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 16, 16, 32)   0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 16, 16, 32)   9248        activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 16, 16, 32)   128         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 16, 16, 32)   544         activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 16, 16, 32)   0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 16, 16, 32)   128         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 16, 16, 32)   0           activation_122[0][0]             \n",
            "                                                                 batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 16, 16, 32)   0           add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 16, 16, 32)   9248        activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 16, 16, 32)   128         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 16, 16, 32)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 16, 16, 32)   9248        activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 16, 16, 32)   128         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 16, 16, 32)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 16, 16, 32)   0           activation_125[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 16, 16, 32)   0           add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 16, 16, 32)   9248        activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 16, 16, 32)   128         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 16, 16, 32)   0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 16, 16, 32)   9248        activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 16, 16, 32)   128         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 16, 16, 32)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 16, 16, 32)   0           activation_128[0][0]             \n",
            "                                                                 activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 16, 16, 32)   0           add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 16, 16, 32)   9248        activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 16, 16, 32)   128         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 16, 16, 32)   0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 16, 16, 32)   9248        activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 16, 16, 32)   128         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 16, 16, 32)   0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 16, 16, 32)   0           activation_131[0][0]             \n",
            "                                                                 activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 16, 16, 32)   0           add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 8, 8, 64)     18496       activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 8, 8, 64)     256         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 8, 64)     0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 8, 8, 64)     36928       activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 8, 8, 64)     256         conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 8, 8, 64)     2112        activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 8, 8, 64)     0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 8, 8, 64)     256         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 8, 8, 64)     0           activation_134[0][0]             \n",
            "                                                                 batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 8, 8, 64)     0           add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 8, 8, 64)     36928       activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 8, 8, 64)     256         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 8, 8, 64)     0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 8, 8, 64)     36928       activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 8, 8, 64)     256         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 8, 8, 64)     0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 8, 8, 64)     0           activation_137[0][0]             \n",
            "                                                                 activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 8, 8, 64)     0           add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 8, 8, 64)     36928       activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 8, 8, 64)     256         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 8, 8, 64)     0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 8, 8, 64)     36928       activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 8, 8, 64)     256         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 8, 8, 64)     0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 8, 8, 64)     0           activation_140[0][0]             \n",
            "                                                                 activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 8, 8, 64)     0           add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 8, 8, 64)     36928       activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 8, 8, 64)     256         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 8, 8, 64)     0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 8, 8, 64)     36928       activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 8, 8, 64)     256         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 8, 8, 64)     0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 8, 8, 64)     0           activation_143[0][0]             \n",
            "                                                                 activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 8, 8, 64)     0           add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 8, 8, 64)     36928       activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 8, 8, 64)     256         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 8, 8, 64)     0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 8, 8, 64)     36928       activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 8, 8, 64)     256         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 8, 8, 64)     0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 8, 8, 64)     0           activation_146[0][0]             \n",
            "                                                                 activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 8, 8, 64)     0           add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 8, 8, 64)     36928       activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 8, 8, 64)     256         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 8, 8, 64)     0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 8, 8, 64)     36928       activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 8, 8, 64)     256         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 8, 8, 64)     0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 8, 8, 64)     0           activation_149[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 8, 8, 64)     0           add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 4, 4, 128)    73856       activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 4, 4, 128)    512         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 4, 4, 128)    0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 4, 4, 128)    147584      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 4, 4, 128)    512         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 4, 4, 128)    8320        activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 4, 4, 128)    0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 4, 4, 128)    512         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 4, 4, 128)    0           activation_152[0][0]             \n",
            "                                                                 batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 4, 4, 128)    0           add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 4, 4, 128)    147584      activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 4, 4, 128)    512         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 4, 4, 128)    0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 4, 4, 128)    147584      activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 4, 4, 128)    512         conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 4, 4, 128)    0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 4, 4, 128)    0           activation_155[0][0]             \n",
            "                                                                 activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 4, 4, 128)    0           add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 4, 4, 128)    147584      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 4, 4, 128)    512         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 4, 4, 128)    0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 4, 4, 128)    147584      activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 4, 4, 128)    512         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 4, 4, 128)    0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 4, 4, 128)    0           activation_158[0][0]             \n",
            "                                                                 activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 4, 4, 128)    0           add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 4, 4, 128)    512         activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 4, 4, 128)    0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 128)          0           activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10)           1290        global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 1,341,514\n",
            "Trainable params: 1,337,002\n",
            "Non-trainable params: 4,512\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k-YQ-q5f5lk4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento y test"
      ]
    },
    {
      "metadata": {
        "id": "YWZx17Tr5sC8",
        "colab_type": "code",
        "outputId": "7099095c-b398-4661-c1f4-a83186883541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6964
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "epochs=200\n",
        "\n",
        "opt = SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
        "resnet.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=testdatagen.flow(x_test, y_test),\n",
        "                            validation_steps=len(x_test) / batch_size,\n",
        "                            callbacks=[lrate_res,chkpoint],\n",
        "                            verbose=2)\n",
        "\n",
        "# scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "#print('Test loss:', scores[0])\n",
        "#print('Test accuracy:', scores[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " - 68s - loss: 1.4919 - acc: 0.4541 - val_loss: 1.9898 - val_acc: 0.4189\n",
            "Epoch 2/200\n",
            " - 58s - loss: 1.1058 - acc: 0.6080 - val_loss: 1.3819 - val_acc: 0.5518\n",
            "Epoch 3/200\n",
            " - 58s - loss: 0.9274 - acc: 0.6703 - val_loss: 1.1506 - val_acc: 0.6250\n",
            "Epoch 4/200\n",
            " - 58s - loss: 0.8172 - acc: 0.7129 - val_loss: 1.6937 - val_acc: 0.5557\n",
            "Epoch 5/200\n",
            " - 58s - loss: 0.7423 - acc: 0.7411 - val_loss: 0.8937 - val_acc: 0.7061\n",
            "Epoch 6/200\n",
            " - 58s - loss: 0.6884 - acc: 0.7599 - val_loss: 1.0419 - val_acc: 0.6559\n",
            "Epoch 7/200\n",
            " - 58s - loss: 0.6338 - acc: 0.7776 - val_loss: 0.9937 - val_acc: 0.6974\n",
            "Epoch 8/200\n",
            " - 58s - loss: 0.6026 - acc: 0.7915 - val_loss: 0.6270 - val_acc: 0.7910\n",
            "Epoch 9/200\n",
            " - 58s - loss: 0.5722 - acc: 0.8014 - val_loss: 0.8890 - val_acc: 0.7164\n",
            "Epoch 10/200\n",
            " - 58s - loss: 0.5478 - acc: 0.8085 - val_loss: 0.9205 - val_acc: 0.7136\n",
            "Epoch 11/200\n",
            " - 58s - loss: 0.5196 - acc: 0.8189 - val_loss: 0.8183 - val_acc: 0.7259\n",
            "Epoch 12/200\n",
            " - 58s - loss: 0.5018 - acc: 0.8251 - val_loss: 0.5966 - val_acc: 0.7954\n",
            "Epoch 13/200\n",
            " - 58s - loss: 0.4829 - acc: 0.8312 - val_loss: 0.6667 - val_acc: 0.7844\n",
            "Epoch 14/200\n",
            " - 58s - loss: 0.4682 - acc: 0.8370 - val_loss: 0.5457 - val_acc: 0.8105\n",
            "Epoch 15/200\n",
            " - 58s - loss: 0.4532 - acc: 0.8427 - val_loss: 0.7616 - val_acc: 0.7638\n",
            "Epoch 16/200\n",
            " - 58s - loss: 0.4402 - acc: 0.8456 - val_loss: 0.5365 - val_acc: 0.8260\n",
            "Epoch 17/200\n",
            " - 58s - loss: 0.4282 - acc: 0.8505 - val_loss: 0.5929 - val_acc: 0.8042\n",
            "Epoch 18/200\n",
            " - 58s - loss: 0.4108 - acc: 0.8583 - val_loss: 0.6090 - val_acc: 0.7994\n",
            "Epoch 19/200\n",
            " - 58s - loss: 0.4007 - acc: 0.8604 - val_loss: 0.8243 - val_acc: 0.7591\n",
            "Epoch 20/200\n",
            " - 58s - loss: 0.3907 - acc: 0.8636 - val_loss: 0.4940 - val_acc: 0.8344\n",
            "Epoch 21/200\n",
            " - 58s - loss: 0.3762 - acc: 0.8676 - val_loss: 0.6797 - val_acc: 0.7892\n",
            "Epoch 22/200\n",
            " - 58s - loss: 0.3687 - acc: 0.8707 - val_loss: 0.6749 - val_acc: 0.7892\n",
            "Epoch 23/200\n",
            " - 58s - loss: 0.3686 - acc: 0.8707 - val_loss: 0.4949 - val_acc: 0.8339\n",
            "Epoch 24/200\n",
            " - 57s - loss: 0.3536 - acc: 0.8761 - val_loss: 0.5632 - val_acc: 0.8145\n",
            "Epoch 25/200\n",
            " - 58s - loss: 0.3426 - acc: 0.8796 - val_loss: 0.7530 - val_acc: 0.7710\n",
            "Epoch 26/200\n",
            " - 58s - loss: 0.3343 - acc: 0.8830 - val_loss: 0.7276 - val_acc: 0.7824\n",
            "Epoch 27/200\n",
            " - 58s - loss: 0.3278 - acc: 0.8848 - val_loss: 0.6119 - val_acc: 0.8184\n",
            "Epoch 28/200\n",
            " - 58s - loss: 0.3207 - acc: 0.8874 - val_loss: 0.5403 - val_acc: 0.8320\n",
            "Epoch 29/200\n",
            " - 58s - loss: 0.3183 - acc: 0.8872 - val_loss: 0.5510 - val_acc: 0.8295\n",
            "Epoch 30/200\n",
            " - 58s - loss: 0.3067 - acc: 0.8919 - val_loss: 0.4818 - val_acc: 0.8473\n",
            "Epoch 31/200\n",
            " - 58s - loss: 0.3005 - acc: 0.8946 - val_loss: 0.5696 - val_acc: 0.8236\n",
            "Epoch 32/200\n",
            " - 58s - loss: 0.2938 - acc: 0.8959 - val_loss: 0.4480 - val_acc: 0.8436\n",
            "Epoch 33/200\n",
            " - 58s - loss: 0.2876 - acc: 0.8985 - val_loss: 0.4500 - val_acc: 0.8576\n",
            "Epoch 34/200\n",
            " - 58s - loss: 0.2859 - acc: 0.8995 - val_loss: 0.7039 - val_acc: 0.8014\n",
            "Epoch 35/200\n",
            " - 58s - loss: 0.2790 - acc: 0.9017 - val_loss: 0.5618 - val_acc: 0.8303\n",
            "Epoch 36/200\n",
            " - 58s - loss: 0.2813 - acc: 0.9012 - val_loss: 0.5247 - val_acc: 0.8483\n",
            "Epoch 37/200\n",
            " - 58s - loss: 0.2676 - acc: 0.9044 - val_loss: 0.5396 - val_acc: 0.8362\n",
            "Epoch 38/200\n",
            " - 58s - loss: 0.2644 - acc: 0.9073 - val_loss: 0.4872 - val_acc: 0.8497\n",
            "Epoch 39/200\n",
            " - 58s - loss: 0.2589 - acc: 0.9091 - val_loss: 0.4836 - val_acc: 0.8592\n",
            "Epoch 40/200\n",
            " - 57s - loss: 0.2563 - acc: 0.9095 - val_loss: 0.4267 - val_acc: 0.8615\n",
            "Epoch 41/200\n",
            " - 58s - loss: 0.2527 - acc: 0.9098 - val_loss: 0.6226 - val_acc: 0.8291\n",
            "Epoch 42/200\n",
            " - 58s - loss: 0.2484 - acc: 0.9127 - val_loss: 0.5098 - val_acc: 0.8374\n",
            "Epoch 43/200\n",
            " - 58s - loss: 0.2421 - acc: 0.9140 - val_loss: 0.4340 - val_acc: 0.8627\n",
            "Epoch 44/200\n",
            " - 58s - loss: 0.2388 - acc: 0.9156 - val_loss: 0.4764 - val_acc: 0.8511\n",
            "Epoch 45/200\n",
            " - 58s - loss: 0.2303 - acc: 0.9183 - val_loss: 0.4440 - val_acc: 0.8576\n",
            "Epoch 46/200\n",
            " - 58s - loss: 0.2301 - acc: 0.9171 - val_loss: 0.4791 - val_acc: 0.8517\n",
            "Epoch 47/200\n",
            " - 58s - loss: 0.2301 - acc: 0.9185 - val_loss: 0.5043 - val_acc: 0.8521\n",
            "Epoch 48/200\n",
            " - 58s - loss: 0.2207 - acc: 0.9213 - val_loss: 0.5523 - val_acc: 0.8352\n",
            "Epoch 49/200\n",
            " - 58s - loss: 0.2224 - acc: 0.9209 - val_loss: 0.7617 - val_acc: 0.8062\n",
            "Epoch 50/200\n",
            " - 58s - loss: 0.2153 - acc: 0.9228 - val_loss: 0.5625 - val_acc: 0.8465\n",
            "Epoch 51/200\n",
            " - 58s - loss: 0.2121 - acc: 0.9242 - val_loss: 0.4620 - val_acc: 0.8536\n",
            "Epoch 52/200\n",
            " - 58s - loss: 0.2070 - acc: 0.9259 - val_loss: 0.4625 - val_acc: 0.8579\n",
            "Epoch 53/200\n",
            " - 58s - loss: 0.2031 - acc: 0.9284 - val_loss: 0.6490 - val_acc: 0.8271\n",
            "Epoch 54/200\n",
            " - 58s - loss: 0.2066 - acc: 0.9271 - val_loss: 0.3892 - val_acc: 0.8849\n",
            "Epoch 55/200\n",
            " - 58s - loss: 0.2032 - acc: 0.9264 - val_loss: 0.3953 - val_acc: 0.8813\n",
            "Epoch 56/200\n",
            " - 58s - loss: 0.1982 - acc: 0.9307 - val_loss: 0.4448 - val_acc: 0.8603\n",
            "Epoch 57/200\n",
            " - 58s - loss: 0.1942 - acc: 0.9306 - val_loss: 0.3834 - val_acc: 0.8821\n",
            "Epoch 58/200\n",
            " - 58s - loss: 0.1905 - acc: 0.9316 - val_loss: 0.4644 - val_acc: 0.8699\n",
            "Epoch 59/200\n",
            " - 58s - loss: 0.1934 - acc: 0.9311 - val_loss: 0.4282 - val_acc: 0.8710\n",
            "Epoch 60/200\n",
            " - 59s - loss: 0.1849 - acc: 0.9345 - val_loss: 0.3934 - val_acc: 0.8742\n",
            "Epoch 61/200\n",
            " - 58s - loss: 0.1870 - acc: 0.9327 - val_loss: 0.4198 - val_acc: 0.8647\n",
            "Epoch 62/200\n",
            " - 58s - loss: 0.1818 - acc: 0.9348 - val_loss: 0.3952 - val_acc: 0.8900\n",
            "Epoch 63/200\n",
            " - 58s - loss: 0.1772 - acc: 0.9363 - val_loss: 0.4356 - val_acc: 0.8762\n",
            "Epoch 64/200\n",
            " - 58s - loss: 0.1778 - acc: 0.9366 - val_loss: 0.5265 - val_acc: 0.8507\n",
            "Epoch 65/200\n",
            " - 58s - loss: 0.1743 - acc: 0.9369 - val_loss: 0.5662 - val_acc: 0.8513\n",
            "Epoch 66/200\n",
            " - 58s - loss: 0.1719 - acc: 0.9392 - val_loss: 0.5391 - val_acc: 0.8509\n",
            "Epoch 67/200\n",
            " - 58s - loss: 0.1740 - acc: 0.9376 - val_loss: 0.4607 - val_acc: 0.8718\n",
            "Epoch 68/200\n",
            " - 58s - loss: 0.1643 - acc: 0.9415 - val_loss: 0.4628 - val_acc: 0.8639\n",
            "Epoch 69/200\n",
            " - 58s - loss: 0.1655 - acc: 0.9406 - val_loss: 0.5566 - val_acc: 0.8540\n",
            "Epoch 70/200\n",
            " - 58s - loss: 0.1651 - acc: 0.9405 - val_loss: 0.4521 - val_acc: 0.8738\n",
            "Epoch 71/200\n",
            " - 58s - loss: 0.1616 - acc: 0.9422 - val_loss: 0.4125 - val_acc: 0.8809\n",
            "Epoch 72/200\n",
            " - 57s - loss: 0.1595 - acc: 0.9436 - val_loss: 0.5418 - val_acc: 0.8471\n",
            "Epoch 73/200\n",
            " - 58s - loss: 0.1601 - acc: 0.9436 - val_loss: 0.5583 - val_acc: 0.8568\n",
            "Epoch 74/200\n",
            " - 58s - loss: 0.1567 - acc: 0.9437 - val_loss: 0.3991 - val_acc: 0.8869\n",
            "Epoch 75/200\n",
            " - 58s - loss: 0.1503 - acc: 0.9455 - val_loss: 0.4894 - val_acc: 0.8572\n",
            "Epoch 76/200\n",
            " - 58s - loss: 0.1541 - acc: 0.9459 - val_loss: 0.4305 - val_acc: 0.8718\n",
            "Epoch 77/200\n",
            " - 58s - loss: 0.1490 - acc: 0.9476 - val_loss: 0.4575 - val_acc: 0.8750\n",
            "Epoch 78/200\n",
            " - 58s - loss: 0.1461 - acc: 0.9473 - val_loss: 0.5105 - val_acc: 0.8616\n",
            "Epoch 79/200\n",
            " - 58s - loss: 0.1456 - acc: 0.9475 - val_loss: 0.4689 - val_acc: 0.8722\n",
            "Epoch 80/200\n",
            " - 58s - loss: 0.1452 - acc: 0.9480 - val_loss: 0.4931 - val_acc: 0.8730\n",
            "Epoch 81/200\n",
            " - 58s - loss: 0.1431 - acc: 0.9488 - val_loss: 0.4538 - val_acc: 0.8703\n",
            "Epoch 82/200\n",
            " - 58s - loss: 0.1176 - acc: 0.9580 - val_loss: 0.2909 - val_acc: 0.9118\n",
            "Epoch 83/200\n",
            " - 58s - loss: 0.0990 - acc: 0.9649 - val_loss: 0.3200 - val_acc: 0.9102\n",
            "Epoch 84/200\n",
            " - 58s - loss: 0.0915 - acc: 0.9676 - val_loss: 0.3330 - val_acc: 0.9017\n",
            "Epoch 85/200\n",
            " - 58s - loss: 0.0902 - acc: 0.9692 - val_loss: 0.3355 - val_acc: 0.9082\n",
            "Epoch 86/200\n",
            " - 58s - loss: 0.0848 - acc: 0.9695 - val_loss: 0.3089 - val_acc: 0.9122\n",
            "Epoch 87/200\n",
            " - 58s - loss: 0.0841 - acc: 0.9708 - val_loss: 0.3597 - val_acc: 0.9094\n",
            "Epoch 88/200\n",
            " - 58s - loss: 0.0825 - acc: 0.9712 - val_loss: 0.3478 - val_acc: 0.9064\n",
            "Epoch 89/200\n",
            " - 58s - loss: 0.0808 - acc: 0.9725 - val_loss: 0.2910 - val_acc: 0.9165\n",
            "Epoch 90/200\n",
            " - 58s - loss: 0.0785 - acc: 0.9729 - val_loss: 0.3620 - val_acc: 0.9051\n",
            "Epoch 91/200\n",
            " - 58s - loss: 0.0749 - acc: 0.9743 - val_loss: 0.3383 - val_acc: 0.9110\n",
            "Epoch 92/200\n",
            " - 58s - loss: 0.0757 - acc: 0.9730 - val_loss: 0.3627 - val_acc: 0.9088\n",
            "Epoch 93/200\n",
            " - 58s - loss: 0.0767 - acc: 0.9735 - val_loss: 0.3541 - val_acc: 0.9122\n",
            "Epoch 94/200\n",
            " - 58s - loss: 0.0747 - acc: 0.9737 - val_loss: 0.3043 - val_acc: 0.9201\n",
            "Epoch 95/200\n",
            " - 58s - loss: 0.0690 - acc: 0.9761 - val_loss: 0.3085 - val_acc: 0.9126\n",
            "Epoch 96/200\n",
            " - 58s - loss: 0.0703 - acc: 0.9751 - val_loss: 0.3380 - val_acc: 0.9061\n",
            "Epoch 97/200\n",
            " - 58s - loss: 0.0706 - acc: 0.9753 - val_loss: 0.3435 - val_acc: 0.9066\n",
            "Epoch 98/200\n",
            " - 58s - loss: 0.0695 - acc: 0.9750 - val_loss: 0.3641 - val_acc: 0.9051\n",
            "Epoch 99/200\n",
            " - 58s - loss: 0.0683 - acc: 0.9762 - val_loss: 0.3317 - val_acc: 0.9181\n",
            "Epoch 100/200\n",
            " - 58s - loss: 0.0684 - acc: 0.9764 - val_loss: 0.3395 - val_acc: 0.9216\n",
            "Epoch 101/200\n",
            " - 58s - loss: 0.0686 - acc: 0.9761 - val_loss: 0.3834 - val_acc: 0.9055\n",
            "Epoch 102/200\n",
            " - 58s - loss: 0.0681 - acc: 0.9759 - val_loss: 0.3100 - val_acc: 0.9169\n",
            "Epoch 103/200\n",
            " - 58s - loss: 0.0664 - acc: 0.9764 - val_loss: 0.3680 - val_acc: 0.9066\n",
            "Epoch 104/200\n",
            " - 57s - loss: 0.0655 - acc: 0.9771 - val_loss: 0.3496 - val_acc: 0.9084\n",
            "Epoch 105/200\n",
            " - 58s - loss: 0.0642 - acc: 0.9774 - val_loss: 0.3555 - val_acc: 0.9130\n",
            "Epoch 106/200\n",
            " - 58s - loss: 0.0632 - acc: 0.9773 - val_loss: 0.3397 - val_acc: 0.9157\n",
            "Epoch 107/200\n",
            " - 58s - loss: 0.0663 - acc: 0.9759 - val_loss: 0.3740 - val_acc: 0.9076\n",
            "Epoch 108/200\n",
            " - 58s - loss: 0.0625 - acc: 0.9783 - val_loss: 0.3764 - val_acc: 0.9043\n",
            "Epoch 109/200\n",
            " - 57s - loss: 0.0620 - acc: 0.9783 - val_loss: 0.3754 - val_acc: 0.9003\n",
            "Epoch 110/200\n",
            " - 58s - loss: 0.0619 - acc: 0.9782 - val_loss: 0.3634 - val_acc: 0.9110\n",
            "Epoch 111/200\n",
            " - 58s - loss: 0.0616 - acc: 0.9788 - val_loss: 0.3290 - val_acc: 0.9192\n",
            "Epoch 112/200\n",
            " - 58s - loss: 0.0603 - acc: 0.9784 - val_loss: 0.3712 - val_acc: 0.9118\n",
            "Epoch 113/200\n",
            " - 58s - loss: 0.0622 - acc: 0.9782 - val_loss: 0.3684 - val_acc: 0.9074\n",
            "Epoch 114/200\n",
            " - 58s - loss: 0.0600 - acc: 0.9794 - val_loss: 0.3665 - val_acc: 0.9066\n",
            "Epoch 115/200\n",
            " - 57s - loss: 0.0591 - acc: 0.9793 - val_loss: 0.3530 - val_acc: 0.9100\n",
            "Epoch 116/200\n",
            " - 58s - loss: 0.0596 - acc: 0.9789 - val_loss: 0.3476 - val_acc: 0.9153\n",
            "Epoch 117/200\n",
            " - 58s - loss: 0.0598 - acc: 0.9797 - val_loss: 0.3538 - val_acc: 0.9138\n",
            "Epoch 118/200\n",
            " - 58s - loss: 0.0593 - acc: 0.9796 - val_loss: 0.3652 - val_acc: 0.9094\n",
            "Epoch 119/200\n",
            " - 58s - loss: 0.0575 - acc: 0.9794 - val_loss: 0.3922 - val_acc: 0.9013\n",
            "Epoch 120/200\n",
            " - 58s - loss: 0.0563 - acc: 0.9796 - val_loss: 0.4021 - val_acc: 0.9098\n",
            "Epoch 121/200\n",
            " - 58s - loss: 0.0586 - acc: 0.9795 - val_loss: 0.3581 - val_acc: 0.9106\n",
            "Epoch 122/200\n",
            " - 58s - loss: 0.0558 - acc: 0.9805 - val_loss: 0.3524 - val_acc: 0.9078\n",
            "Epoch 123/200\n",
            " - 58s - loss: 0.0545 - acc: 0.9810 - val_loss: 0.3580 - val_acc: 0.9112\n",
            "Epoch 124/200\n",
            " - 58s - loss: 0.0542 - acc: 0.9808 - val_loss: 0.3855 - val_acc: 0.9142\n",
            "Epoch 125/200\n",
            " - 58s - loss: 0.0557 - acc: 0.9808 - val_loss: 0.3155 - val_acc: 0.9142\n",
            "Epoch 126/200\n",
            " - 58s - loss: 0.0538 - acc: 0.9811 - val_loss: 0.3998 - val_acc: 0.8999\n",
            "Epoch 127/200\n",
            " - 58s - loss: 0.0523 - acc: 0.9819 - val_loss: 0.3574 - val_acc: 0.9196\n",
            "Epoch 128/200\n",
            " - 58s - loss: 0.0563 - acc: 0.9803 - val_loss: 0.3880 - val_acc: 0.9059\n",
            "Epoch 129/200\n",
            " - 57s - loss: 0.0536 - acc: 0.9812 - val_loss: 0.3520 - val_acc: 0.9150\n",
            "Epoch 130/200\n",
            " - 57s - loss: 0.0522 - acc: 0.9818 - val_loss: 0.3803 - val_acc: 0.9051\n",
            "Epoch 131/200\n",
            " - 57s - loss: 0.0533 - acc: 0.9815 - val_loss: 0.3518 - val_acc: 0.9128\n",
            "Epoch 132/200\n",
            " - 57s - loss: 0.0526 - acc: 0.9820 - val_loss: 0.2947 - val_acc: 0.9225\n",
            "Epoch 133/200\n",
            " - 57s - loss: 0.0545 - acc: 0.9809 - val_loss: 0.3681 - val_acc: 0.9094\n",
            "Epoch 134/200\n",
            " - 57s - loss: 0.0529 - acc: 0.9812 - val_loss: 0.3848 - val_acc: 0.9055\n",
            "Epoch 135/200\n",
            " - 57s - loss: 0.0540 - acc: 0.9812 - val_loss: 0.4016 - val_acc: 0.9049\n",
            "Epoch 136/200\n",
            " - 57s - loss: 0.0539 - acc: 0.9808 - val_loss: 0.3263 - val_acc: 0.9189\n",
            "Epoch 137/200\n",
            " - 57s - loss: 0.0531 - acc: 0.9809 - val_loss: 0.3777 - val_acc: 0.9106\n",
            "Epoch 138/200\n",
            " - 58s - loss: 0.0524 - acc: 0.9813 - val_loss: 0.3508 - val_acc: 0.9157\n",
            "Epoch 139/200\n",
            " - 58s - loss: 0.0516 - acc: 0.9819 - val_loss: 0.3810 - val_acc: 0.9045\n",
            "Epoch 140/200\n",
            " - 57s - loss: 0.0519 - acc: 0.9816 - val_loss: 0.3573 - val_acc: 0.9157\n",
            "Epoch 141/200\n",
            " - 57s - loss: 0.0541 - acc: 0.9811 - val_loss: 0.3777 - val_acc: 0.9070\n",
            "Epoch 142/200\n",
            " - 58s - loss: 0.0507 - acc: 0.9820 - val_loss: 0.3644 - val_acc: 0.9126\n",
            "Epoch 143/200\n",
            " - 58s - loss: 0.0529 - acc: 0.9814 - val_loss: 0.3526 - val_acc: 0.9092\n",
            "Epoch 144/200\n",
            " - 58s - loss: 0.0526 - acc: 0.9815 - val_loss: 0.3670 - val_acc: 0.9082\n",
            "Epoch 145/200\n",
            " - 58s - loss: 0.0528 - acc: 0.9812 - val_loss: 0.3634 - val_acc: 0.9102\n",
            "Epoch 146/200\n",
            " - 58s - loss: 0.0530 - acc: 0.9818 - val_loss: 0.3964 - val_acc: 0.9094\n",
            "Epoch 147/200\n",
            " - 57s - loss: 0.0530 - acc: 0.9814 - val_loss: 0.3267 - val_acc: 0.9204\n",
            "Epoch 148/200\n",
            " - 58s - loss: 0.0490 - acc: 0.9825 - val_loss: 0.3356 - val_acc: 0.9173\n",
            "Epoch 149/200\n",
            " - 58s - loss: 0.0515 - acc: 0.9820 - val_loss: 0.3521 - val_acc: 0.9185\n",
            "Epoch 150/200\n",
            " - 58s - loss: 0.0525 - acc: 0.9818 - val_loss: 0.3991 - val_acc: 0.9047\n",
            "Epoch 151/200\n",
            " - 58s - loss: 0.0507 - acc: 0.9824 - val_loss: 0.3932 - val_acc: 0.8981\n",
            "Epoch 152/200\n",
            " - 58s - loss: 0.0532 - acc: 0.9813 - val_loss: 0.3635 - val_acc: 0.9146\n",
            "Epoch 153/200\n",
            " - 58s - loss: 0.0507 - acc: 0.9819 - val_loss: 0.3671 - val_acc: 0.9142\n",
            "Epoch 154/200\n",
            " - 58s - loss: 0.0506 - acc: 0.9825 - val_loss: 0.3756 - val_acc: 0.9086\n",
            "Epoch 155/200\n",
            " - 58s - loss: 0.0495 - acc: 0.9834 - val_loss: 0.3333 - val_acc: 0.9100\n",
            "Epoch 156/200\n",
            " - 58s - loss: 0.0528 - acc: 0.9816 - val_loss: 0.3845 - val_acc: 0.9039\n",
            "Epoch 157/200\n",
            " - 58s - loss: 0.0532 - acc: 0.9815 - val_loss: 0.3817 - val_acc: 0.9114\n",
            "Epoch 158/200\n",
            " - 57s - loss: 0.0499 - acc: 0.9821 - val_loss: 0.3665 - val_acc: 0.9090\n",
            "Epoch 159/200\n",
            " - 58s - loss: 0.0502 - acc: 0.9824 - val_loss: 0.3290 - val_acc: 0.9196\n",
            "Epoch 160/200\n",
            " - 58s - loss: 0.0505 - acc: 0.9819 - val_loss: 0.3480 - val_acc: 0.9066\n",
            "Epoch 161/200\n",
            " - 58s - loss: 0.0514 - acc: 0.9814 - val_loss: 0.4055 - val_acc: 0.9098\n",
            "Epoch 162/200\n",
            " - 58s - loss: 0.0507 - acc: 0.9822 - val_loss: 0.3711 - val_acc: 0.9134\n",
            "Epoch 163/200\n",
            " - 58s - loss: 0.0515 - acc: 0.9821 - val_loss: 0.3762 - val_acc: 0.9144\n",
            "Epoch 164/200\n",
            " - 58s - loss: 0.0505 - acc: 0.9822 - val_loss: 0.3898 - val_acc: 0.9066\n",
            "Epoch 165/200\n",
            " - 58s - loss: 0.0526 - acc: 0.9812 - val_loss: 0.3404 - val_acc: 0.9161\n",
            "Epoch 166/200\n",
            " - 58s - loss: 0.0516 - acc: 0.9830 - val_loss: 0.3598 - val_acc: 0.9098\n",
            "Epoch 167/200\n",
            " - 58s - loss: 0.0529 - acc: 0.9812 - val_loss: 0.3590 - val_acc: 0.9112\n",
            "Epoch 168/200\n",
            " - 58s - loss: 0.0521 - acc: 0.9822 - val_loss: 0.3894 - val_acc: 0.8995\n",
            "Epoch 169/200\n",
            " - 58s - loss: 0.0539 - acc: 0.9804 - val_loss: 0.3938 - val_acc: 0.9102\n",
            "Epoch 170/200\n",
            " - 58s - loss: 0.0492 - acc: 0.9826 - val_loss: 0.3529 - val_acc: 0.9185\n",
            "Epoch 171/200\n",
            " - 58s - loss: 0.0497 - acc: 0.9824 - val_loss: 0.3324 - val_acc: 0.9208\n",
            "Epoch 172/200\n",
            " - 58s - loss: 0.0492 - acc: 0.9826 - val_loss: 0.3754 - val_acc: 0.9114\n",
            "Epoch 173/200\n",
            " - 58s - loss: 0.0498 - acc: 0.9831 - val_loss: 0.3742 - val_acc: 0.9114\n",
            "Epoch 174/200\n",
            " - 57s - loss: 0.0516 - acc: 0.9815 - val_loss: 0.3849 - val_acc: 0.9019\n",
            "Epoch 175/200\n",
            " - 58s - loss: 0.0492 - acc: 0.9834 - val_loss: 0.4037 - val_acc: 0.9013\n",
            "Epoch 176/200\n",
            " - 58s - loss: 0.0496 - acc: 0.9828 - val_loss: 0.3775 - val_acc: 0.9138\n",
            "Epoch 177/200\n",
            " - 58s - loss: 0.0486 - acc: 0.9833 - val_loss: 0.3268 - val_acc: 0.9142\n",
            "Epoch 178/200\n",
            " - 58s - loss: 0.0500 - acc: 0.9834 - val_loss: 0.3922 - val_acc: 0.9122\n",
            "Epoch 179/200\n",
            " - 58s - loss: 0.0506 - acc: 0.9819 - val_loss: 0.3561 - val_acc: 0.9132\n",
            "Epoch 180/200\n",
            " - 58s - loss: 0.0524 - acc: 0.9825 - val_loss: 0.3392 - val_acc: 0.9134\n",
            "Epoch 181/200\n",
            " - 58s - loss: 0.0513 - acc: 0.9815 - val_loss: 0.3831 - val_acc: 0.9110\n",
            "Epoch 182/200\n",
            " - 58s - loss: 0.0481 - acc: 0.9834 - val_loss: 0.3687 - val_acc: 0.9106\n",
            "Epoch 183/200\n",
            " - 58s - loss: 0.0495 - acc: 0.9829 - val_loss: 0.4041 - val_acc: 0.9053\n",
            "Epoch 184/200\n",
            " - 58s - loss: 0.0485 - acc: 0.9832 - val_loss: 0.3428 - val_acc: 0.9197\n",
            "Epoch 185/200\n",
            " - 57s - loss: 0.0520 - acc: 0.9822 - val_loss: 0.3943 - val_acc: 0.9055\n",
            "Epoch 186/200\n",
            " - 58s - loss: 0.0505 - acc: 0.9831 - val_loss: 0.3620 - val_acc: 0.9106\n",
            "Epoch 187/200\n",
            " - 58s - loss: 0.0468 - acc: 0.9836 - val_loss: 0.3546 - val_acc: 0.9160\n",
            "Epoch 188/200\n",
            " - 58s - loss: 0.0508 - acc: 0.9820 - val_loss: 0.3879 - val_acc: 0.9126\n",
            "Epoch 189/200\n",
            " - 58s - loss: 0.0503 - acc: 0.9822 - val_loss: 0.4168 - val_acc: 0.9047\n",
            "Epoch 190/200\n",
            " - 57s - loss: 0.0467 - acc: 0.9833 - val_loss: 0.3081 - val_acc: 0.9189\n",
            "Epoch 191/200\n",
            " - 58s - loss: 0.0508 - acc: 0.9823 - val_loss: 0.3995 - val_acc: 0.9076\n",
            "Epoch 192/200\n",
            " - 58s - loss: 0.0500 - acc: 0.9827 - val_loss: 0.3674 - val_acc: 0.9094\n",
            "Epoch 193/200\n",
            " - 58s - loss: 0.0495 - acc: 0.9832 - val_loss: 0.3627 - val_acc: 0.9169\n",
            "Epoch 194/200\n",
            " - 58s - loss: 0.0482 - acc: 0.9834 - val_loss: 0.3685 - val_acc: 0.9118\n",
            "Epoch 195/200\n",
            " - 58s - loss: 0.0500 - acc: 0.9822 - val_loss: 0.3853 - val_acc: 0.9037\n",
            "Epoch 196/200\n",
            " - 58s - loss: 0.0496 - acc: 0.9826 - val_loss: 0.3535 - val_acc: 0.9197\n",
            "Epoch 197/200\n",
            " - 58s - loss: 0.0477 - acc: 0.9838 - val_loss: 0.3960 - val_acc: 0.9035\n",
            "Epoch 198/200\n",
            " - 58s - loss: 0.0456 - acc: 0.9837 - val_loss: 0.3382 - val_acc: 0.9169\n",
            "Epoch 199/200\n",
            " - 58s - loss: 0.0486 - acc: 0.9829 - val_loss: 0.4022 - val_acc: 0.9072\n",
            "Epoch 200/200\n",
            " - 58s - loss: 0.0474 - acc: 0.9833 - val_loss: 0.3781 - val_acc: 0.9110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bDZYyg7gpN3g",
        "colab_type": "code",
        "outputId": "62f606e2-d8d5-4921-eade-433ad73501a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('ResNet accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}